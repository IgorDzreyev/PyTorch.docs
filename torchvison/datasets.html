

<!DOCTYPE html>
<html lang="en" >
<head>
  <meta charset="utf-8">
  <script type="text/javascript">

      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-90545585-1']);
      _gaq.push(['_trackPageview']);

      (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();
    </script>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torchvision.datasets &mdash; Torchvision master documentation</title>
  

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript">
          var DOCUMENTATION_OPTIONS = {
              URL_ROOT:'./',
              VERSION:'master',
              LANGUAGE:'None',
              COLLAPSE_INDEX:false,
              FILE_SUFFIX:'.html',
              HAS_SOURCE:  true,
              SOURCELINK_SUFFIX: '.txt'
          };
      </script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato" type="text/css" />
  <link rel="stylesheet" href="_static/css/pytorch_theme.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="torchvision.io" href="io.html" />
    <link rel="prev" title="torchvision" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html">
          

          
            
            <img src="_static/pytorch-logo-dark.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                master (0.5.0 )
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Package Reference</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">torchvision.datasets</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#mnist">MNIST</a></li>
<li class="toctree-l2"><a class="reference internal" href="#fashion-mnist">Fashion-MNIST</a></li>
<li class="toctree-l2"><a class="reference internal" href="#kmnist">KMNIST</a></li>
<li class="toctree-l2"><a class="reference internal" href="#emnist">EMNIST</a></li>
<li class="toctree-l2"><a class="reference internal" href="#qmnist">QMNIST</a></li>
<li class="toctree-l2"><a class="reference internal" href="#fakedata">FakeData</a></li>
<li class="toctree-l2"><a class="reference internal" href="#coco">COCO</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#captions">Captions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#detection">Detection</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#lsun">LSUN</a></li>
<li class="toctree-l2"><a class="reference internal" href="#imagefolder">ImageFolder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#datasetfolder">DatasetFolder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#imagenet">ImageNet</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cifar">CIFAR</a></li>
<li class="toctree-l2"><a class="reference internal" href="#stl10">STL10</a></li>
<li class="toctree-l2"><a class="reference internal" href="#svhn">SVHN</a></li>
<li class="toctree-l2"><a class="reference internal" href="#phototour">PhotoTour</a></li>
<li class="toctree-l2"><a class="reference internal" href="#sbu">SBU</a></li>
<li class="toctree-l2"><a class="reference internal" href="#flickr">Flickr</a></li>
<li class="toctree-l2"><a class="reference internal" href="#voc">VOC</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cityscapes">Cityscapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="#sbd">SBD</a></li>
<li class="toctree-l2"><a class="reference internal" href="#usps">USPS</a></li>
<li class="toctree-l2"><a class="reference internal" href="#kinetics-400">Kinetics-400</a></li>
<li class="toctree-l2"><a class="reference internal" href="#hmdb51">HMDB51</a></li>
<li class="toctree-l2"><a class="reference internal" href="#ucf101">UCF101</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="io.html">torchvision.io</a><ul>
<li class="toctree-l2"><a class="reference internal" href="io.html#video">Video</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="models.html">torchvision.models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="models.html#classification">Classification</a><ul>
<li class="toctree-l3"><a class="reference internal" href="models.html#id1">Alexnet</a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#id2">VGG</a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#id10">ResNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#id15">SqueezeNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#id16">DenseNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#inception-v3">Inception v3</a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#id23">GoogLeNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#shufflenet-v2">ShuffleNet v2</a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#mobilenet-v2">MobileNet v2</a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#id27">ResNext</a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#wide-resnet">Wide ResNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#id30">MNASNet</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="models.html#semantic-segmentation">Semantic Segmentation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="models.html#fully-convolutional-networks">Fully Convolutional Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#deeplabv3">DeepLabV3</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="models.html#object-detection-instance-segmentation-and-person-keypoint-detection">Object Detection, Instance Segmentation and Person Keypoint Detection</a><ul>
<li class="toctree-l3"><a class="reference internal" href="models.html#runtime-characteristics">Runtime characteristics</a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#faster-r-cnn">Faster R-CNN</a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#mask-r-cnn">Mask R-CNN</a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#keypoint-r-cnn">Keypoint R-CNN</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="models.html#video-classification">Video classification</a><ul>
<li class="toctree-l3"><a class="reference internal" href="models.html#resnet-3d">ResNet 3D</a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#resnet-mixed-convolution">ResNet Mixed Convolution</a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#resnet-2-1-d">ResNet (2+1)D</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ops.html">torchvision.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="transforms.html">torchvision.transforms</a><ul>
<li class="toctree-l2"><a class="reference internal" href="transforms.html#transforms-on-pil-image">Transforms on PIL Image</a></li>
<li class="toctree-l2"><a class="reference internal" href="transforms.html#transforms-on-torch-tensor">Transforms on torch.*Tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="transforms.html#conversion-transforms">Conversion Transforms</a></li>
<li class="toctree-l2"><a class="reference internal" href="transforms.html#generic-transforms">Generic Transforms</a></li>
<li class="toctree-l2"><a class="reference internal" href="transforms.html#functional-transforms">Functional Transforms</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">torchvision.utils</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Torchvision</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>torchvision.datasets</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/datasets.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="torchvision-datasets">
<h1>torchvision.datasets<a class="headerlink" href="#torchvision-datasets" title="Permalink to this headline">¶</a></h1>
<p>All datasets are subclasses of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.Dataset</span></code>
i.e, they have <code class="docutils literal notranslate"><span class="pre">__getitem__</span></code> and <code class="docutils literal notranslate"><span class="pre">__len__</span></code> methods implemented.
Hence, they can all be passed to a <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code>
which can load multiple samples parallelly using <code class="docutils literal notranslate"><span class="pre">torch.multiprocessing</span></code> workers.
For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">imagenet_data</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">ImageNet</span><span class="p">(</span><span class="s1">&#39;path/to/imagenet_root/&#39;</span><span class="p">)</span>
<span class="n">data_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">imagenet_data</span><span class="p">,</span>
                                          <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                                          <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                          <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">nThreads</span><span class="p">)</span>
</pre></div>
</div>
<p>The following datasets are available:</p>
<div class="contents local topic" id="datasets">
<p class="topic-title">Datasets</p>
<ul class="simple">
<li><a class="reference internal" href="#mnist" id="id17">MNIST</a></li>
<li><a class="reference internal" href="#fashion-mnist" id="id18">Fashion-MNIST</a></li>
<li><a class="reference internal" href="#kmnist" id="id19">KMNIST</a></li>
<li><a class="reference internal" href="#emnist" id="id20">EMNIST</a></li>
<li><a class="reference internal" href="#qmnist" id="id21">QMNIST</a></li>
<li><a class="reference internal" href="#fakedata" id="id22">FakeData</a></li>
<li><a class="reference internal" href="#coco" id="id23">COCO</a><ul>
<li><a class="reference internal" href="#captions" id="id24">Captions</a></li>
<li><a class="reference internal" href="#detection" id="id25">Detection</a></li>
</ul>
</li>
<li><a class="reference internal" href="#lsun" id="id26">LSUN</a></li>
<li><a class="reference internal" href="#imagefolder" id="id27">ImageFolder</a></li>
<li><a class="reference internal" href="#datasetfolder" id="id28">DatasetFolder</a></li>
<li><a class="reference internal" href="#imagenet" id="id29">ImageNet</a></li>
<li><a class="reference internal" href="#cifar" id="id30">CIFAR</a></li>
<li><a class="reference internal" href="#stl10" id="id31">STL10</a></li>
<li><a class="reference internal" href="#svhn" id="id32">SVHN</a></li>
<li><a class="reference internal" href="#phototour" id="id33">PhotoTour</a></li>
<li><a class="reference internal" href="#sbu" id="id34">SBU</a></li>
<li><a class="reference internal" href="#flickr" id="id35">Flickr</a></li>
<li><a class="reference internal" href="#voc" id="id36">VOC</a></li>
<li><a class="reference internal" href="#cityscapes" id="id37">Cityscapes</a></li>
<li><a class="reference internal" href="#sbd" id="id38">SBD</a></li>
<li><a class="reference internal" href="#usps" id="id39">USPS</a></li>
<li><a class="reference internal" href="#kinetics-400" id="id40">Kinetics-400</a></li>
<li><a class="reference internal" href="#hmdb51" id="id41">HMDB51</a></li>
<li><a class="reference internal" href="#ucf101" id="id42">UCF101</a></li>
</ul>
</div>
<p>All the datasets have almost similar API. They all have two common arguments:
<code class="docutils literal notranslate"><span class="pre">transform</span></code> and  <code class="docutils literal notranslate"><span class="pre">target_transform</span></code> to transform the input and target respectively.</p>
<div class="section" id="mnist">
<h2><a class="toc-backref" href="#id17">MNIST</a><a class="headerlink" href="#mnist" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.MNIST">
<em class="property">class </em><code class="descclassname">torchvision.datasets.</code><code class="descname">MNIST</code><span class="sig-paren">(</span><em>root</em>, <em>train=True</em>, <em>transform=None</em>, <em>target_transform=None</em>, <em>download=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/mnist.html#MNIST"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.MNIST" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://yann.lecun.com/exdb/mnist/">MNIST</a> Dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>root</strong> (<em>string</em>) – Root directory of dataset where <code class="docutils literal notranslate"><span class="pre">MNIST/processed/training.pt</span></code>
and  <code class="docutils literal notranslate"><span class="pre">MNIST/processed/test.pt</span></code> exist.</li>
<li><strong>train</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If True, creates dataset from <code class="docutils literal notranslate"><span class="pre">training.pt</span></code>,
otherwise from <code class="docutils literal notranslate"><span class="pre">test.pt</span></code>.</li>
<li><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and
puts it in root directory. If dataset is already downloaded, it is not
downloaded again.</li>
<li><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></li>
<li><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="fashion-mnist">
<h2><a class="toc-backref" href="#id18">Fashion-MNIST</a><a class="headerlink" href="#fashion-mnist" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.FashionMNIST">
<em class="property">class </em><code class="descclassname">torchvision.datasets.</code><code class="descname">FashionMNIST</code><span class="sig-paren">(</span><em>root</em>, <em>train=True</em>, <em>transform=None</em>, <em>target_transform=None</em>, <em>download=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/mnist.html#FashionMNIST"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.FashionMNIST" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://github.com/zalandoresearch/fashion-mnist">Fashion-MNIST</a> Dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>root</strong> (<em>string</em>) – Root directory of dataset where <code class="docutils literal notranslate"><span class="pre">Fashion-MNIST/processed/training.pt</span></code>
and  <code class="docutils literal notranslate"><span class="pre">Fashion-MNIST/processed/test.pt</span></code> exist.</li>
<li><strong>train</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If True, creates dataset from <code class="docutils literal notranslate"><span class="pre">training.pt</span></code>,
otherwise from <code class="docutils literal notranslate"><span class="pre">test.pt</span></code>.</li>
<li><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and
puts it in root directory. If dataset is already downloaded, it is not
downloaded again.</li>
<li><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></li>
<li><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="kmnist">
<h2><a class="toc-backref" href="#id19">KMNIST</a><a class="headerlink" href="#kmnist" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.KMNIST">
<em class="property">class </em><code class="descclassname">torchvision.datasets.</code><code class="descname">KMNIST</code><span class="sig-paren">(</span><em>root</em>, <em>train=True</em>, <em>transform=None</em>, <em>target_transform=None</em>, <em>download=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/mnist.html#KMNIST"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.KMNIST" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://github.com/rois-codh/kmnist">Kuzushiji-MNIST</a> Dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>root</strong> (<em>string</em>) – Root directory of dataset where <code class="docutils literal notranslate"><span class="pre">KMNIST/processed/training.pt</span></code>
and  <code class="docutils literal notranslate"><span class="pre">KMNIST/processed/test.pt</span></code> exist.</li>
<li><strong>train</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If True, creates dataset from <code class="docutils literal notranslate"><span class="pre">training.pt</span></code>,
otherwise from <code class="docutils literal notranslate"><span class="pre">test.pt</span></code>.</li>
<li><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and
puts it in root directory. If dataset is already downloaded, it is not
downloaded again.</li>
<li><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></li>
<li><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="emnist">
<h2><a class="toc-backref" href="#id20">EMNIST</a><a class="headerlink" href="#emnist" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.EMNIST">
<em class="property">class </em><code class="descclassname">torchvision.datasets.</code><code class="descname">EMNIST</code><span class="sig-paren">(</span><em>root</em>, <em>split</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/mnist.html#EMNIST"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.EMNIST" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.westernsydney.edu.au/bens/home/reproducible_research/emnist">EMNIST</a> Dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>root</strong> (<em>string</em>) – Root directory of dataset where <code class="docutils literal notranslate"><span class="pre">EMNIST/processed/training.pt</span></code>
and  <code class="docutils literal notranslate"><span class="pre">EMNIST/processed/test.pt</span></code> exist.</li>
<li><strong>split</strong> (<em>string</em>) – The dataset has 6 different splits: <code class="docutils literal notranslate"><span class="pre">byclass</span></code>, <code class="docutils literal notranslate"><span class="pre">bymerge</span></code>,
<code class="docutils literal notranslate"><span class="pre">balanced</span></code>, <code class="docutils literal notranslate"><span class="pre">letters</span></code>, <code class="docutils literal notranslate"><span class="pre">digits</span></code> and <code class="docutils literal notranslate"><span class="pre">mnist</span></code>. This argument specifies
which one to use.</li>
<li><strong>train</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If True, creates dataset from <code class="docutils literal notranslate"><span class="pre">training.pt</span></code>,
otherwise from <code class="docutils literal notranslate"><span class="pre">test.pt</span></code>.</li>
<li><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and
puts it in root directory. If dataset is already downloaded, it is not
downloaded again.</li>
<li><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></li>
<li><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="qmnist">
<h2><a class="toc-backref" href="#id21">QMNIST</a><a class="headerlink" href="#qmnist" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.QMNIST">
<em class="property">class </em><code class="descclassname">torchvision.datasets.</code><code class="descname">QMNIST</code><span class="sig-paren">(</span><em>root</em>, <em>what=None</em>, <em>compat=True</em>, <em>train=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/mnist.html#QMNIST"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.QMNIST" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://github.com/facebookresearch/qmnist">QMNIST</a> Dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>root</strong> (<em>string</em>) – Root directory of dataset whose <a href="#id5"><span class="problematic" id="id6">``</span></a>processed’’
subdir contains torch binary files with the datasets.</li>
<li><strong>what</strong> (<em>string</em><em>,</em><em>optional</em>) – Can be ‘train’, ‘test’, ‘test10k’,
‘test50k’, or ‘nist’ for respectively the mnist compatible
training set, the 60k qmnist testing set, the 10k qmnist
examples that match the mnist testing set, the 50k
remaining qmnist testing examples, or all the nist
digits. The default is to select ‘train’ or ‘test’
according to the compatibility argument ‘train’.</li>
<li><strong>compat</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>,</em><em>optional</em>) – A boolean that says whether the target
for each example is class number (for compatibility with
the MNIST dataloader) or a torch vector containing the
full qmnist information. Default=True.</li>
<li><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from
the internet and puts it in root directory. If dataset is
already downloaded, it is not downloaded again.</li>
<li><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that
takes in an PIL image and returns a transformed
version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></li>
<li><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform
that takes in the target and transforms it.</li>
<li><strong>train</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>,</em><em>optional</em><em>,</em><em>compatibility</em>) – When argument ‘what’ is
not specified, this boolean decides whether to load the
training set ot the testing set.  Default: True.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="fakedata">
<h2><a class="toc-backref" href="#id22">FakeData</a><a class="headerlink" href="#fakedata" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.FakeData">
<em class="property">class </em><code class="descclassname">torchvision.datasets.</code><code class="descname">FakeData</code><span class="sig-paren">(</span><em>size=1000</em>, <em>image_size=(3</em>, <em>224</em>, <em>224)</em>, <em>num_classes=10</em>, <em>transform=None</em>, <em>target_transform=None</em>, <em>random_offset=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/fakedata.html#FakeData"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.FakeData" title="Permalink to this definition">¶</a></dt>
<dd><p>A fake dataset that returns randomly generated images and returns them as PIL images</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – Size of the dataset. Default: 1000 images</li>
<li><strong>image_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>, </em><em>optional</em>) – Size if the returned images. Default: (3, 224, 224)</li>
<li><strong>num_classes</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – Number of classes in the datset. Default: 10</li>
<li><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></li>
<li><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</li>
<li><strong>random_offset</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Offsets the index-based random seed used to
generate each image. Default: 0</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="coco">
<h2><a class="toc-backref" href="#id23">COCO</a><a class="headerlink" href="#coco" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">These require the <a class="reference external" href="https://github.com/pdollar/coco/tree/master/PythonAPI">COCO API to be installed</a></p>
</div>
<div class="section" id="captions">
<h3><a class="toc-backref" href="#id24">Captions</a><a class="headerlink" href="#captions" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torchvision.datasets.CocoCaptions">
<em class="property">class </em><code class="descclassname">torchvision.datasets.</code><code class="descname">CocoCaptions</code><span class="sig-paren">(</span><em>root</em>, <em>annFile</em>, <em>transform=None</em>, <em>target_transform=None</em>, <em>transforms=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/coco.html#CocoCaptions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.CocoCaptions" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://mscoco.org/dataset/#captions-challenge2015">MS Coco Captions</a> Dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>root</strong> (<em>string</em>) – Root directory where images are downloaded to.</li>
<li><strong>annFile</strong> (<em>string</em>) – Path to json annotation file.</li>
<li><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.ToTensor</span></code></li>
<li><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</li>
<li><strong>transforms</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes input sample and its target as entry
and returns a transformed version.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torchvision.datasets</span> <span class="k">as</span> <span class="nn">dset</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="n">cap</span> <span class="o">=</span> <span class="n">dset</span><span class="o">.</span><span class="n">CocoCaptions</span><span class="p">(</span><span class="n">root</span> <span class="o">=</span> <span class="s1">&#39;dir where images are&#39;</span><span class="p">,</span>
                        <span class="n">annFile</span> <span class="o">=</span> <span class="s1">&#39;json annotation file&#39;</span><span class="p">,</span>
                        <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of samples: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cap</span><span class="p">))</span>
<span class="n">img</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">cap</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="c1"># load 4th sample</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Image Size: &quot;</span><span class="p">,</span> <span class="n">img</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Number</span> <span class="n">of</span> <span class="n">samples</span><span class="p">:</span> <span class="mi">82783</span>
<span class="n">Image</span> <span class="n">Size</span><span class="p">:</span> <span class="p">(</span><span class="mi">3</span><span class="n">L</span><span class="p">,</span> <span class="mi">427</span><span class="n">L</span><span class="p">,</span> <span class="mi">640</span><span class="n">L</span><span class="p">)</span>
<span class="p">[</span><span class="sa">u</span><span class="s1">&#39;A plane emitting smoke stream flying over a mountain.&#39;</span><span class="p">,</span>
<span class="sa">u</span><span class="s1">&#39;A plane darts across a bright blue sky behind a mountain covered in snow&#39;</span><span class="p">,</span>
<span class="sa">u</span><span class="s1">&#39;A plane leaves a contrail above the snowy mountain top.&#39;</span><span class="p">,</span>
<span class="sa">u</span><span class="s1">&#39;A mountain that has a plane flying overheard in the distance.&#39;</span><span class="p">,</span>
<span class="sa">u</span><span class="s1">&#39;A mountain view with a plume of smoke in the background&#39;</span><span class="p">]</span>
</pre></div>
</div>
<dl class="method">
<dt id="torchvision.datasets.CocoCaptions.__getitem__">
<code class="descname">__getitem__</code><span class="sig-paren">(</span><em>index</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/coco.html#CocoCaptions.__getitem__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.CocoCaptions.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">Tuple (image, target). target is a list of captions for the image.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="detection">
<h3><a class="toc-backref" href="#id25">Detection</a><a class="headerlink" href="#detection" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torchvision.datasets.CocoDetection">
<em class="property">class </em><code class="descclassname">torchvision.datasets.</code><code class="descname">CocoDetection</code><span class="sig-paren">(</span><em>root</em>, <em>annFile</em>, <em>transform=None</em>, <em>target_transform=None</em>, <em>transforms=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/coco.html#CocoDetection"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.CocoDetection" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://mscoco.org/dataset/#detections-challenge2016">MS Coco Detection</a> Dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>root</strong> (<em>string</em>) – Root directory where images are downloaded to.</li>
<li><strong>annFile</strong> (<em>string</em>) – Path to json annotation file.</li>
<li><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.ToTensor</span></code></li>
<li><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</li>
<li><strong>transforms</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes input sample and its target as entry
and returns a transformed version.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="torchvision.datasets.CocoDetection.__getitem__">
<code class="descname">__getitem__</code><span class="sig-paren">(</span><em>index</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/coco.html#CocoDetection.__getitem__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.CocoDetection.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">Tuple (image, target). target is the object returned by <code class="docutils literal notranslate"><span class="pre">coco.loadAnns</span></code>.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="lsun">
<h2><a class="toc-backref" href="#id26">LSUN</a><a class="headerlink" href="#lsun" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.LSUN">
<em class="property">class </em><code class="descclassname">torchvision.datasets.</code><code class="descname">LSUN</code><span class="sig-paren">(</span><em>root</em>, <em>classes='train'</em>, <em>transform=None</em>, <em>target_transform=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/lsun.html#LSUN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.LSUN" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.yf.io/p/lsun">LSUN</a> dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>root</strong> (<em>string</em>) – Root directory for the database files.</li>
<li><strong>classes</strong> (<em>string</em><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a>) – One of {‘train’, ‘val’, ‘test’} or a list of
categories to load. e,g. [‘bedroom_train’, ‘church_train’].</li>
<li><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></li>
<li><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="torchvision.datasets.LSUN.__getitem__">
<code class="descname">__getitem__</code><span class="sig-paren">(</span><em>index</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/lsun.html#LSUN.__getitem__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.LSUN.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">Tuple (image, target) where target is the index of the target category.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="imagefolder">
<h2><a class="toc-backref" href="#id27">ImageFolder</a><a class="headerlink" href="#imagefolder" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.ImageFolder">
<em class="property">class </em><code class="descclassname">torchvision.datasets.</code><code class="descname">ImageFolder</code><span class="sig-paren">(</span><em>root</em>, <em>transform=None</em>, <em>target_transform=None</em>, <em>loader=&lt;function default_loader&gt;</em>, <em>is_valid_file=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/folder.html#ImageFolder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.ImageFolder" title="Permalink to this definition">¶</a></dt>
<dd><p>A generic data loader where the images are arranged in this way:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">root</span><span class="o">/</span><span class="n">dog</span><span class="o">/</span><span class="n">xxx</span><span class="o">.</span><span class="n">png</span>
<span class="n">root</span><span class="o">/</span><span class="n">dog</span><span class="o">/</span><span class="n">xxy</span><span class="o">.</span><span class="n">png</span>
<span class="n">root</span><span class="o">/</span><span class="n">dog</span><span class="o">/</span><span class="n">xxz</span><span class="o">.</span><span class="n">png</span>

<span class="n">root</span><span class="o">/</span><span class="n">cat</span><span class="o">/</span><span class="mf">123.</span><span class="n">png</span>
<span class="n">root</span><span class="o">/</span><span class="n">cat</span><span class="o">/</span><span class="n">nsdf3</span><span class="o">.</span><span class="n">png</span>
<span class="n">root</span><span class="o">/</span><span class="n">cat</span><span class="o">/</span><span class="n">asd932_</span><span class="o">.</span><span class="n">png</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>root</strong> (<em>string</em>) – Root directory path.</li>
<li><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></li>
<li><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</li>
<li><strong>loader</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function to load an image given its path.</li>
<li><strong>is_valid_file</strong> – A function that takes path of an Image file
and check if the file is a valid file (used to check of corrupt files)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="torchvision.datasets.ImageFolder.__getitem__">
<code class="descname">__getitem__</code><span class="sig-paren">(</span><em>index</em><span class="sig-paren">)</span><a class="headerlink" href="#torchvision.datasets.ImageFolder.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">(sample, target) where target is class_index of the target class.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="datasetfolder">
<h2><a class="toc-backref" href="#id28">DatasetFolder</a><a class="headerlink" href="#datasetfolder" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.DatasetFolder">
<em class="property">class </em><code class="descclassname">torchvision.datasets.</code><code class="descname">DatasetFolder</code><span class="sig-paren">(</span><em>root</em>, <em>loader</em>, <em>extensions=None</em>, <em>transform=None</em>, <em>target_transform=None</em>, <em>is_valid_file=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/folder.html#DatasetFolder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.DatasetFolder" title="Permalink to this definition">¶</a></dt>
<dd><p>A generic data loader where the samples are arranged in this way:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">root</span><span class="o">/</span><span class="n">class_x</span><span class="o">/</span><span class="n">xxx</span><span class="o">.</span><span class="n">ext</span>
<span class="n">root</span><span class="o">/</span><span class="n">class_x</span><span class="o">/</span><span class="n">xxy</span><span class="o">.</span><span class="n">ext</span>
<span class="n">root</span><span class="o">/</span><span class="n">class_x</span><span class="o">/</span><span class="n">xxz</span><span class="o">.</span><span class="n">ext</span>

<span class="n">root</span><span class="o">/</span><span class="n">class_y</span><span class="o">/</span><span class="mf">123.</span><span class="n">ext</span>
<span class="n">root</span><span class="o">/</span><span class="n">class_y</span><span class="o">/</span><span class="n">nsdf3</span><span class="o">.</span><span class="n">ext</span>
<span class="n">root</span><span class="o">/</span><span class="n">class_y</span><span class="o">/</span><span class="n">asd932_</span><span class="o">.</span><span class="n">ext</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>root</strong> (<em>string</em>) – Root directory path.</li>
<li><strong>loader</strong> (<em>callable</em>) – A function to load a sample given its path.</li>
<li><strong>extensions</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><em>string</em><em>]</em>) – A list of allowed extensions.
both extensions and is_valid_file should not be passed.</li>
<li><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in
a sample and returns a transformed version.
E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code> for images.</li>
<li><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes
in the target and transforms it.</li>
<li><strong>is_valid_file</strong> – A function that takes path of a file
and check if the file is a valid file (used to check of corrupt files)
both extensions and is_valid_file should not be passed.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="torchvision.datasets.DatasetFolder.__getitem__">
<code class="descname">__getitem__</code><span class="sig-paren">(</span><em>index</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/folder.html#DatasetFolder.__getitem__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.DatasetFolder.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">(sample, target) where target is class_index of the target class.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="imagenet">
<h2><a class="toc-backref" href="#id29">ImageNet</a><a class="headerlink" href="#imagenet" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.ImageNet">
<em class="property">class </em><code class="descclassname">torchvision.datasets.</code><code class="descname">ImageNet</code><span class="sig-paren">(</span><em>root</em>, <em>split='train'</em>, <em>download=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/imagenet.html#ImageNet"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.ImageNet" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://image-net.org/">ImageNet</a> 2012 Classification Dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>root</strong> (<em>string</em>) – Root directory of the ImageNet Dataset.</li>
<li><strong>split</strong> (<em>string</em><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>, or <code class="docutils literal notranslate"><span class="pre">val</span></code>.</li>
<li><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></li>
<li><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</li>
<li><strong>loader</strong> – A function to load an image given its path.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This requires <cite>scipy</cite> to be installed</p>
</div>
</div>
<div class="section" id="cifar">
<h2><a class="toc-backref" href="#id30">CIFAR</a><a class="headerlink" href="#cifar" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.CIFAR10">
<em class="property">class </em><code class="descclassname">torchvision.datasets.</code><code class="descname">CIFAR10</code><span class="sig-paren">(</span><em>root</em>, <em>train=True</em>, <em>transform=None</em>, <em>target_transform=None</em>, <em>download=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/cifar.html#CIFAR10"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.CIFAR10" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR10</a> Dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>root</strong> (<em>string</em>) – Root directory of dataset where directory
<code class="docutils literal notranslate"><span class="pre">cifar-10-batches-py</span></code> exists or will be saved to if download is set to True.</li>
<li><strong>train</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If True, creates dataset from training set, otherwise
creates from test set.</li>
<li><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></li>
<li><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</li>
<li><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and
puts it in root directory. If dataset is already downloaded, it is not
downloaded again.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="torchvision.datasets.CIFAR10.__getitem__">
<code class="descname">__getitem__</code><span class="sig-paren">(</span><em>index</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/cifar.html#CIFAR10.__getitem__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.CIFAR10.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">(image, target) where target is index of the target class.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchvision.datasets.CIFAR100">
<em class="property">class </em><code class="descclassname">torchvision.datasets.</code><code class="descname">CIFAR100</code><span class="sig-paren">(</span><em>root</em>, <em>train=True</em>, <em>transform=None</em>, <em>target_transform=None</em>, <em>download=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/cifar.html#CIFAR100"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.CIFAR100" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR100</a> Dataset.</p>
<p>This is a subclass of the <cite>CIFAR10</cite> Dataset.</p>
</dd></dl>

</div>
<div class="section" id="stl10">
<h2><a class="toc-backref" href="#id31">STL10</a><a class="headerlink" href="#stl10" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.STL10">
<em class="property">class </em><code class="descclassname">torchvision.datasets.</code><code class="descname">STL10</code><span class="sig-paren">(</span><em>root</em>, <em>split='train'</em>, <em>folds=None</em>, <em>transform=None</em>, <em>target_transform=None</em>, <em>download=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/stl10.html#STL10"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.STL10" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://cs.stanford.edu/~acoates/stl10/">STL10</a> Dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>root</strong> (<em>string</em>) – Root directory of dataset where directory
<code class="docutils literal notranslate"><span class="pre">stl10_binary</span></code> exists.</li>
<li><strong>split</strong> (<em>string</em>) – One of {‘train’, ‘test’, ‘unlabeled’, ‘train+unlabeled’}.
Accordingly dataset is selected.</li>
<li><strong>folds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – <p>One of {0-9} or None.
For training, loads one of the 10 pre-defined folds of 1k samples for the</p>
<blockquote>
<div>standard evaluation procedure. If no value is passed, loads the 5k samples.</div></blockquote>
</li>
<li><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></li>
<li><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</li>
<li><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and
puts it in root directory. If dataset is already downloaded, it is not
downloaded again.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="torchvision.datasets.STL10.__getitem__">
<code class="descname">__getitem__</code><span class="sig-paren">(</span><em>index</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/stl10.html#STL10.__getitem__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.STL10.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">(image, target) where target is index of the target class.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="svhn">
<h2><a class="toc-backref" href="#id32">SVHN</a><a class="headerlink" href="#svhn" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.SVHN">
<em class="property">class </em><code class="descclassname">torchvision.datasets.</code><code class="descname">SVHN</code><span class="sig-paren">(</span><em>root</em>, <em>split='train'</em>, <em>transform=None</em>, <em>target_transform=None</em>, <em>download=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/svhn.html#SVHN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.SVHN" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://ufldl.stanford.edu/housenumbers/">SVHN</a> Dataset.
Note: The SVHN dataset assigns the label <cite>10</cite> to the digit <cite>0</cite>. However, in this Dataset,
we assign the label <cite>0</cite> to the digit <cite>0</cite> to be compatible with PyTorch loss functions which
expect the class labels to be in the range <cite>[0, C-1]</cite></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>root</strong> (<em>string</em>) – Root directory of dataset where directory
<code class="docutils literal notranslate"><span class="pre">SVHN</span></code> exists.</li>
<li><strong>split</strong> (<em>string</em>) – One of {‘train’, ‘test’, ‘extra’}.
Accordingly dataset is selected. ‘extra’ is Extra training set.</li>
<li><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></li>
<li><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</li>
<li><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and
puts it in root directory. If dataset is already downloaded, it is not
downloaded again.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="torchvision.datasets.SVHN.__getitem__">
<code class="descname">__getitem__</code><span class="sig-paren">(</span><em>index</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/svhn.html#SVHN.__getitem__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.SVHN.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">(image, target) where target is index of the target class.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="phototour">
<h2><a class="toc-backref" href="#id33">PhotoTour</a><a class="headerlink" href="#phototour" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.PhotoTour">
<em class="property">class </em><code class="descclassname">torchvision.datasets.</code><code class="descname">PhotoTour</code><span class="sig-paren">(</span><em>root</em>, <em>name</em>, <em>train=True</em>, <em>transform=None</em>, <em>download=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/phototour.html#PhotoTour"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.PhotoTour" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://phototour.cs.washington.edu/patches/default.htm">Learning Local Image Descriptors Data</a> Dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>root</strong> (<em>string</em>) – Root directory where images are.</li>
<li><strong>name</strong> (<em>string</em>) – Name of the dataset to load.</li>
<li><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version.</li>
<li><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and
puts it in root directory. If dataset is already downloaded, it is not
downloaded again.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="torchvision.datasets.PhotoTour.__getitem__">
<code class="descname">__getitem__</code><span class="sig-paren">(</span><em>index</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/phototour.html#PhotoTour.__getitem__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.PhotoTour.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">(data1, data2, matches)</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="sbu">
<h2><a class="toc-backref" href="#id34">SBU</a><a class="headerlink" href="#sbu" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.SBU">
<em class="property">class </em><code class="descclassname">torchvision.datasets.</code><code class="descname">SBU</code><span class="sig-paren">(</span><em>root</em>, <em>transform=None</em>, <em>target_transform=None</em>, <em>download=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/sbu.html#SBU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.SBU" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://www.cs.virginia.edu/~vicente/sbucaptions/">SBU Captioned Photo</a> Dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>root</strong> (<em>string</em>) – Root directory of dataset where tarball
<code class="docutils literal notranslate"><span class="pre">SBUCaptionedPhotoDataset.tar.gz</span></code> exists.</li>
<li><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in a PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></li>
<li><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</li>
<li><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If True, downloads the dataset from the internet and
puts it in root directory. If dataset is already downloaded, it is not
downloaded again.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="torchvision.datasets.SBU.__getitem__">
<code class="descname">__getitem__</code><span class="sig-paren">(</span><em>index</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/sbu.html#SBU.__getitem__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.SBU.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">(image, target) where target is a caption for the photo.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="flickr">
<h2><a class="toc-backref" href="#id35">Flickr</a><a class="headerlink" href="#flickr" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.Flickr8k">
<em class="property">class </em><code class="descclassname">torchvision.datasets.</code><code class="descname">Flickr8k</code><span class="sig-paren">(</span><em>root</em>, <em>ann_file</em>, <em>transform=None</em>, <em>target_transform=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/flickr.html#Flickr8k"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.Flickr8k" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://nlp.cs.illinois.edu/HockenmaierGroup/8k-pictures.html">Flickr8k Entities</a> Dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>root</strong> (<em>string</em>) – Root directory where images are downloaded to.</li>
<li><strong>ann_file</strong> (<em>string</em>) – Path to annotation file.</li>
<li><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in a PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.ToTensor</span></code></li>
<li><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="torchvision.datasets.Flickr8k.__getitem__">
<code class="descname">__getitem__</code><span class="sig-paren">(</span><em>index</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/flickr.html#Flickr8k.__getitem__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.Flickr8k.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">Tuple (image, target). target is a list of captions for the image.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchvision.datasets.Flickr30k">
<em class="property">class </em><code class="descclassname">torchvision.datasets.</code><code class="descname">Flickr30k</code><span class="sig-paren">(</span><em>root</em>, <em>ann_file</em>, <em>transform=None</em>, <em>target_transform=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/flickr.html#Flickr30k"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.Flickr30k" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://web.engr.illinois.edu/~bplumme2/Flickr30kEntities/">Flickr30k Entities</a> Dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>root</strong> (<em>string</em>) – Root directory where images are downloaded to.</li>
<li><strong>ann_file</strong> (<em>string</em>) – Path to annotation file.</li>
<li><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in a PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.ToTensor</span></code></li>
<li><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="torchvision.datasets.Flickr30k.__getitem__">
<code class="descname">__getitem__</code><span class="sig-paren">(</span><em>index</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/flickr.html#Flickr30k.__getitem__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.Flickr30k.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">Tuple (image, target). target is a list of captions for the image.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="voc">
<h2><a class="toc-backref" href="#id36">VOC</a><a class="headerlink" href="#voc" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.VOCSegmentation">
<em class="property">class </em><code class="descclassname">torchvision.datasets.</code><code class="descname">VOCSegmentation</code><span class="sig-paren">(</span><em>root</em>, <em>year='2012'</em>, <em>image_set='train'</em>, <em>download=False</em>, <em>transform=None</em>, <em>target_transform=None</em>, <em>transforms=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/voc.html#VOCSegmentation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.VOCSegmentation" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://host.robots.ox.ac.uk/pascal/VOC/">Pascal VOC</a> Segmentation Dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>root</strong> (<em>string</em>) – Root directory of the VOC Dataset.</li>
<li><strong>year</strong> (<em>string</em><em>, </em><em>optional</em>) – The dataset year, supports years 2007 to 2012.</li>
<li><strong>image_set</strong> (<em>string</em><em>, </em><em>optional</em>) – Select the image_set to use, <code class="docutils literal notranslate"><span class="pre">train</span></code>, <code class="docutils literal notranslate"><span class="pre">trainval</span></code> or <code class="docutils literal notranslate"><span class="pre">val</span></code></li>
<li><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and
puts it in root directory. If dataset is already downloaded, it is not
downloaded again.</li>
<li><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></li>
<li><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</li>
<li><strong>transforms</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes input sample and its target as entry
and returns a transformed version.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="torchvision.datasets.VOCSegmentation.__getitem__">
<code class="descname">__getitem__</code><span class="sig-paren">(</span><em>index</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/voc.html#VOCSegmentation.__getitem__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.VOCSegmentation.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">(image, target) where target is the image segmentation.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchvision.datasets.VOCDetection">
<em class="property">class </em><code class="descclassname">torchvision.datasets.</code><code class="descname">VOCDetection</code><span class="sig-paren">(</span><em>root</em>, <em>year='2012'</em>, <em>image_set='train'</em>, <em>download=False</em>, <em>transform=None</em>, <em>target_transform=None</em>, <em>transforms=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/voc.html#VOCDetection"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.VOCDetection" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://host.robots.ox.ac.uk/pascal/VOC/">Pascal VOC</a> Detection Dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>root</strong> (<em>string</em>) – Root directory of the VOC Dataset.</li>
<li><strong>year</strong> (<em>string</em><em>, </em><em>optional</em>) – The dataset year, supports years 2007 to 2012.</li>
<li><strong>image_set</strong> (<em>string</em><em>, </em><em>optional</em>) – Select the image_set to use, <code class="docutils literal notranslate"><span class="pre">train</span></code>, <code class="docutils literal notranslate"><span class="pre">trainval</span></code> or <code class="docutils literal notranslate"><span class="pre">val</span></code></li>
<li><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and
puts it in root directory. If dataset is already downloaded, it is not
downloaded again.
(default: alphabetic indexing of VOC’s 20 classes).</li>
<li><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></li>
<li><strong>target_transform</strong> (<em>callable</em><em>, </em><em>required</em>) – A function/transform that takes in the
target and transforms it.</li>
<li><strong>transforms</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes input sample and its target as entry
and returns a transformed version.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="torchvision.datasets.VOCDetection.__getitem__">
<code class="descname">__getitem__</code><span class="sig-paren">(</span><em>index</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/voc.html#VOCDetection.__getitem__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.VOCDetection.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">(image, target) where target is a dictionary of the XML tree.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="cityscapes">
<h2><a class="toc-backref" href="#id37">Cityscapes</a><a class="headerlink" href="#cityscapes" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Requires Cityscape to be downloaded.</p>
</div>
<dl class="class">
<dt id="torchvision.datasets.Cityscapes">
<em class="property">class </em><code class="descclassname">torchvision.datasets.</code><code class="descname">Cityscapes</code><span class="sig-paren">(</span><em>root</em>, <em>split='train'</em>, <em>mode='fine'</em>, <em>target_type='instance'</em>, <em>transform=None</em>, <em>target_transform=None</em>, <em>transforms=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/cityscapes.html#Cityscapes"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.Cityscapes" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://www.cityscapes-dataset.com/">Cityscapes</a> Dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>root</strong> (<em>string</em>) – Root directory of dataset where directory <code class="docutils literal notranslate"><span class="pre">leftImg8bit</span></code>
and <code class="docutils literal notranslate"><span class="pre">gtFine</span></code> or <code class="docutils literal notranslate"><span class="pre">gtCoarse</span></code> are located.</li>
<li><strong>split</strong> (<em>string</em><em>, </em><em>optional</em>) – The image split to use, <code class="docutils literal notranslate"><span class="pre">train</span></code>, <code class="docutils literal notranslate"><span class="pre">test</span></code> or <code class="docutils literal notranslate"><span class="pre">val</span></code> if mode=”gtFine”
otherwise <code class="docutils literal notranslate"><span class="pre">train</span></code>, <code class="docutils literal notranslate"><span class="pre">train_extra</span></code> or <code class="docutils literal notranslate"><span class="pre">val</span></code></li>
<li><strong>mode</strong> (<em>string</em><em>, </em><em>optional</em>) – The quality mode to use, <code class="docutils literal notranslate"><span class="pre">gtFine</span></code> or <code class="docutils literal notranslate"><span class="pre">gtCoarse</span></code></li>
<li><strong>target_type</strong> (<em>string</em><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>, </em><em>optional</em>) – Type of target to use, <code class="docutils literal notranslate"><span class="pre">instance</span></code>, <code class="docutils literal notranslate"><span class="pre">semantic</span></code>, <code class="docutils literal notranslate"><span class="pre">polygon</span></code>
or <code class="docutils literal notranslate"><span class="pre">color</span></code>. Can also be a list to output a tuple with all specified target types.</li>
<li><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in a PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></li>
<li><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</li>
<li><strong>transforms</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes input sample and its target as entry
and returns a transformed version.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<p>Get semantic segmentation target</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">Cityscapes</span><span class="p">(</span><span class="s1">&#39;./data/cityscapes&#39;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fine&#39;</span><span class="p">,</span>
                     <span class="n">target_type</span><span class="o">=</span><span class="s1">&#39;semantic&#39;</span><span class="p">)</span>

<span class="n">img</span><span class="p">,</span> <span class="n">smnt</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>Get multiple targets</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">Cityscapes</span><span class="p">(</span><span class="s1">&#39;./data/cityscapes&#39;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fine&#39;</span><span class="p">,</span>
                     <span class="n">target_type</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;instance&#39;</span><span class="p">,</span> <span class="s1">&#39;color&#39;</span><span class="p">,</span> <span class="s1">&#39;polygon&#39;</span><span class="p">])</span>

<span class="n">img</span><span class="p">,</span> <span class="p">(</span><span class="n">inst</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">poly</span><span class="p">)</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>Validate on the “coarse” set</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">Cityscapes</span><span class="p">(</span><span class="s1">&#39;./data/cityscapes&#39;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s1">&#39;val&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;coarse&#39;</span><span class="p">,</span>
                     <span class="n">target_type</span><span class="o">=</span><span class="s1">&#39;semantic&#39;</span><span class="p">)</span>

<span class="n">img</span><span class="p">,</span> <span class="n">smnt</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<dl class="method">
<dt id="torchvision.datasets.Cityscapes.__getitem__">
<code class="descname">__getitem__</code><span class="sig-paren">(</span><em>index</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/cityscapes.html#Cityscapes.__getitem__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.Cityscapes.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">(image, target) where target is a tuple of all target types if target_type is a list with more
than one item. Otherwise target is a json object if target_type=”polygon”, else the image segmentation.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="sbd">
<h2><a class="toc-backref" href="#id38">SBD</a><a class="headerlink" href="#sbd" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.SBDataset">
<em class="property">class </em><code class="descclassname">torchvision.datasets.</code><code class="descname">SBDataset</code><span class="sig-paren">(</span><em>root</em>, <em>image_set='train'</em>, <em>mode='boundaries'</em>, <em>download=False</em>, <em>transforms=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/sbd.html#SBDataset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.SBDataset" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://home.bharathh.info/pubs/codes/SBD/download.html">Semantic Boundaries Dataset</a></p>
<p>The SBD currently contains annotations from 11355 images taken from the PASCAL VOC 2011 dataset.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Please note that the train and val splits included with this dataset are different from
the splits in the PASCAL VOC dataset. In particular some “train” images might be part of
VOC2012 val.
If you are interested in testing on VOC 2012 val, then use <cite>image_set=’train_noval’</cite>,
which excludes all val images.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This class needs <a class="reference external" href="https://docs.scipy.org/doc/">scipy</a> to load target files from <cite>.mat</cite> format.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>root</strong> (<em>string</em>) – Root directory of the Semantic Boundaries Dataset</li>
<li><strong>image_set</strong> (<em>string</em><em>, </em><em>optional</em>) – Select the image_set to use, <code class="docutils literal notranslate"><span class="pre">train</span></code>, <code class="docutils literal notranslate"><span class="pre">val</span></code> or <code class="docutils literal notranslate"><span class="pre">train_noval</span></code>.
Image set <code class="docutils literal notranslate"><span class="pre">train_noval</span></code> excludes VOC 2012 val images.</li>
<li><strong>mode</strong> (<em>string</em><em>, </em><em>optional</em>) – Select target type. Possible values ‘boundaries’ or ‘segmentation’.
In case of ‘boundaries’, the target is an array of shape <cite>[num_classes, H, W]</cite>,
where <cite>num_classes=20</cite>.</li>
<li><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and
puts it in root directory. If dataset is already downloaded, it is not
downloaded again.</li>
<li><strong>transforms</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes input sample and its target as entry
and returns a transformed version. Input sample is PIL image and target is a numpy array
if <cite>mode=’boundaries’</cite> or PIL image if <cite>mode=’segmentation’</cite>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="usps">
<h2><a class="toc-backref" href="#id39">USPS</a><a class="headerlink" href="#usps" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.USPS">
<em class="property">class </em><code class="descclassname">torchvision.datasets.</code><code class="descname">USPS</code><span class="sig-paren">(</span><em>root</em>, <em>train=True</em>, <em>transform=None</em>, <em>target_transform=None</em>, <em>download=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/usps.html#USPS"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.USPS" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#usps">USPS</a> Dataset.
The data-format is : [label [index:value ]*256 n] * num_lines, where <code class="docutils literal notranslate"><span class="pre">label</span></code> lies in <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">10]</span></code>.
The value for each pixel lies in <code class="docutils literal notranslate"><span class="pre">[-1,</span> <span class="pre">1]</span></code>. Here we transform the <code class="docutils literal notranslate"><span class="pre">label</span></code> into <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">9]</span></code>
and make pixel values in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">255]</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>root</strong> (<em>string</em>) – Root directory of dataset to store``USPS`` data files.</li>
<li><strong>train</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If True, creates dataset from <code class="docutils literal notranslate"><span class="pre">usps.bz2</span></code>,
otherwise from <code class="docutils literal notranslate"><span class="pre">usps.t.bz2</span></code>.</li>
<li><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></li>
<li><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the
target and transforms it.</li>
<li><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and
puts it in root directory. If dataset is already downloaded, it is not
downloaded again.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="torchvision.datasets.USPS.__getitem__">
<code class="descname">__getitem__</code><span class="sig-paren">(</span><em>index</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/usps.html#USPS.__getitem__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.USPS.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">(image, target) where target is index of the target class.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="kinetics-400">
<h2><a class="toc-backref" href="#id40">Kinetics-400</a><a class="headerlink" href="#kinetics-400" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.Kinetics400">
<em class="property">class </em><code class="descclassname">torchvision.datasets.</code><code class="descname">Kinetics400</code><span class="sig-paren">(</span><em>root</em>, <em>frames_per_clip</em>, <em>step_between_clips=1</em>, <em>frame_rate=None</em>, <em>extensions=('avi'</em>, <em>)</em>, <em>transform=None</em>, <em>_precomputed_metadata=None</em>, <em>num_workers=1</em>, <em>_video_width=0</em>, <em>_video_height=0</em>, <em>_video_min_dimension=0</em>, <em>_audio_samples=0</em>, <em>_audio_channels=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/kinetics.html#Kinetics400"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.Kinetics400" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://deepmind.com/research/open-source/open-source-datasets/kinetics/">Kinetics-400</a>
dataset.</p>
<p>Kinetics-400 is an action recognition video dataset.
This dataset consider every video as a collection of video clips of fixed size, specified
by <code class="docutils literal notranslate"><span class="pre">frames_per_clip</span></code>, where the step in frames between each clip is given by
<code class="docutils literal notranslate"><span class="pre">step_between_clips</span></code>.</p>
<p>To give an example, for 2 videos with 10 and 15 frames respectively, if <code class="docutils literal notranslate"><span class="pre">frames_per_clip=5</span></code>
and <code class="docutils literal notranslate"><span class="pre">step_between_clips=5</span></code>, the dataset size will be (2 + 3) = 5, where the first two
elements will come from video 1, and the next three elements from video 2.
Note that we drop clips which do not have exactly <code class="docutils literal notranslate"><span class="pre">frames_per_clip</span></code> elements, so not all
frames in a video might be present.</p>
<p>Internally, it uses a VideoClips object to handle clip creation.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>root</strong> (<em>string</em>) – Root directory of the Kinetics-400 Dataset.</li>
<li><strong>frames_per_clip</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – number of frames in a clip</li>
<li><strong>step_between_clips</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – number of frames between each clip</li>
<li><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in a TxHxWxC video
and returns a transformed version.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><p>the <cite>T</cite> video frames
audio(Tensor[K, L]): the audio frames, where <cite>K</cite> is the number of channels</p>
<blockquote>
<div><p>and <cite>L</cite> is the number of points</p>
</div></blockquote>
<p>label (int): class of the video clip</p>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">video (Tensor[T, H, W, C])</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="hmdb51">
<h2><a class="toc-backref" href="#id41">HMDB51</a><a class="headerlink" href="#hmdb51" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.HMDB51">
<em class="property">class </em><code class="descclassname">torchvision.datasets.</code><code class="descname">HMDB51</code><span class="sig-paren">(</span><em>root</em>, <em>annotation_path</em>, <em>frames_per_clip</em>, <em>step_between_clips=1</em>, <em>frame_rate=None</em>, <em>fold=1</em>, <em>train=True</em>, <em>transform=None</em>, <em>_precomputed_metadata=None</em>, <em>num_workers=1</em>, <em>_video_width=0</em>, <em>_video_height=0</em>, <em>_video_min_dimension=0</em>, <em>_audio_samples=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/hmdb51.html#HMDB51"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.HMDB51" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/">HMDB51</a>
dataset.</p>
<p>HMDB51 is an action recognition video dataset.
This dataset consider every video as a collection of video clips of fixed size, specified
by <code class="docutils literal notranslate"><span class="pre">frames_per_clip</span></code>, where the step in frames between each clip is given by
<code class="docutils literal notranslate"><span class="pre">step_between_clips</span></code>.</p>
<p>To give an example, for 2 videos with 10 and 15 frames respectively, if <code class="docutils literal notranslate"><span class="pre">frames_per_clip=5</span></code>
and <code class="docutils literal notranslate"><span class="pre">step_between_clips=5</span></code>, the dataset size will be (2 + 3) = 5, where the first two
elements will come from video 1, and the next three elements from video 2.
Note that we drop clips which do not have exactly <code class="docutils literal notranslate"><span class="pre">frames_per_clip</span></code> elements, so not all
frames in a video might be present.</p>
<p>Internally, it uses a VideoClips object to handle clip creation.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>root</strong> (<em>string</em>) – Root directory of the HMDB51 Dataset.</li>
<li><strong>annotation_path</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – path to the folder containing the split files</li>
<li><strong>frames_per_clip</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – number of frames in a clip.</li>
<li><strong>step_between_clips</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – number of frames between each clip.</li>
<li><strong>fold</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – which fold to use. Should be between 1 and 3.</li>
<li><strong>train</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, creates a dataset from the train split,
otherwise from the <code class="docutils literal notranslate"><span class="pre">test</span></code> split.</li>
<li><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in a TxHxWxC video
and returns a transformed version.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><p>the <cite>T</cite> video frames
audio(Tensor[K, L]): the audio frames, where <cite>K</cite> is the number of channels</p>
<blockquote>
<div><p>and <cite>L</cite> is the number of points</p>
</div></blockquote>
<p>label (int): class of the video clip</p>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">video (Tensor[T, H, W, C])</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="ucf101">
<h2><a class="toc-backref" href="#id42">UCF101</a><a class="headerlink" href="#ucf101" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchvision.datasets.UCF101">
<em class="property">class </em><code class="descclassname">torchvision.datasets.</code><code class="descname">UCF101</code><span class="sig-paren">(</span><em>root</em>, <em>annotation_path</em>, <em>frames_per_clip</em>, <em>step_between_clips=1</em>, <em>frame_rate=None</em>, <em>fold=1</em>, <em>train=True</em>, <em>transform=None</em>, <em>_precomputed_metadata=None</em>, <em>num_workers=1</em>, <em>_video_width=0</em>, <em>_video_height=0</em>, <em>_video_min_dimension=0</em>, <em>_audio_samples=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchvision/datasets/ucf101.html#UCF101"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchvision.datasets.UCF101" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.crcv.ucf.edu/data/UCF101.php">UCF101</a> dataset.</p>
<p>UCF101 is an action recognition video dataset.
This dataset consider every video as a collection of video clips of fixed size, specified
by <code class="docutils literal notranslate"><span class="pre">frames_per_clip</span></code>, where the step in frames between each clip is given by
<code class="docutils literal notranslate"><span class="pre">step_between_clips</span></code>.</p>
<p>To give an example, for 2 videos with 10 and 15 frames respectively, if <code class="docutils literal notranslate"><span class="pre">frames_per_clip=5</span></code>
and <code class="docutils literal notranslate"><span class="pre">step_between_clips=5</span></code>, the dataset size will be (2 + 3) = 5, where the first two
elements will come from video 1, and the next three elements from video 2.
Note that we drop clips which do not have exactly <code class="docutils literal notranslate"><span class="pre">frames_per_clip</span></code> elements, so not all
frames in a video might be present.</p>
<p>Internally, it uses a VideoClips object to handle clip creation.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>root</strong> (<em>string</em>) – Root directory of the UCF101 Dataset.</li>
<li><strong>annotation_path</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – path to the folder containing the split files</li>
<li><strong>frames_per_clip</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – number of frames in a clip.</li>
<li><strong>step_between_clips</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – number of frames between each clip.</li>
<li><strong>fold</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – which fold to use. Should be between 1 and 3.</li>
<li><strong>train</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, creates a dataset from the train split,
otherwise from the <code class="docutils literal notranslate"><span class="pre">test</span></code> split.</li>
<li><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in a TxHxWxC video
and returns a transformed version.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><p>the <cite>T</cite> video frames
audio(Tensor[K, L]): the audio frames, where <cite>K</cite> is the number of channels</p>
<blockquote>
<div><p>and <cite>L</cite> is the number of points</p>
</div></blockquote>
<p>label (int): class of the video clip</p>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">video (Tensor[T, H, W, C])</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="io.html" class="btn btn-neutral float-right" title="torchvision.io" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral float-left" title="torchvision" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2017, Torch Contributors

    </p>
  </div>
    
    
      Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>